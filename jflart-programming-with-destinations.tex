\documentclass[english]{jflart}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{minted}
\usepackage{etoolbox,xpatch}
\usepackage{ tipa }
\usepackage[normalem]{ulem}
\usepackage[backend=biber, style=alphabetic]{biblatex}
\addbibresource{bibliography.bib}

% Numéro et année des JFLAs visées par l'article, obligatoire.
\jfla{35}{2024}

\title{Programming with destinations in Haskell}
% Un titre plus court, optionnel.
%\titlerunning{Du bon usage de~\texttt{jflart.cls}}

% Auteurs, liste non abrégée.
\author[1]{Thomas Bagrel}
% \author[2]{Cunégonde Martin}
% \author[2]{Odoacre Contempierre}
% Une liste d'auteurs abrégée à utiliser à l'intérieur de l'article.
\authorrunning{Bagrel}

% Affiliations des auteurs
\affil[1]{INRIA/LORIA, Vand\oe{}uvre-lès-Nancy, 54500, France}
\affil[2]{TWEAG, Paris, 75012, France}

% Une commande définie par l'utilisateur
\newcommand{\cmd}[1]{\texttt{\textbackslash {#1}}}
\newcommand{\mpar}{\text{\,\textramshorns\,}}
\newcommand{\dest}{-\prec}
\usepackage{newunicodechar}
\newunicodechar{⊸}{\ensuremath{\multimap}}
\newunicodechar{→}{\ensuremath{\to}}
\newunicodechar{⇒}{\ensuremath{\Rightarrow}}
\newunicodechar{∀}{\ensuremath{\forall}}
\makeatletter
\AtBeginEnvironment{minted}{\dontdofcolorbox}
\def\dontdofcolorbox{\renewcommand\fcolorbox[4][]{##4}}
\xpatchcmd{\inputminted}{\minted@fvset}{\minted@fvset\dontdofcolorbox}{}{}
\xpatchcmd{\mintinline}{\minted@fvset}{\minted@fvset\dontdofcolorbox}{}{} % see https://tex.stackexchange.com/a/401250/
\makeatother

\begin{document}

\maketitle

\begin{abstract}
Destination-passing style programming introduces destinations, which represents the address of a write-once memory cell. Those destinations can be passed around as function parameters, and thus enable the caller of a function to keep control over memory allocation: the body of the called function will just be responsible of filling that memory cell. This is especially useful in functional programming languages such as Haskell, in which the body of a function is typically responsible for allocation of the result value.

Programming with destination in Haskell is an interesting way to improve performance of critical parts of some programs, without sacrificing memory warranties. Indeed, thanks to a linearly-typed API we designed, a write-once memory cell cannot be left uninitialized before being read, and is still disposed of by the garbage collector when it is not in use anymore, eliminating the risk of uninitialized read, memory leak, or double-free errors that can arise when memory is handled manually.

With the implementation of destinations for Haskell through compact regions we provide in this article, we reach a 15-40\% improvement over memory allocation in a simple parser example, and 0-50\% improvement in run time. We also provide a few examples of programs that can be implemented in a tail-recursive fashion thanks to destinations, which is crucial for performance in strict contexts.

Safety proofs for the API are not provided in this article though, and will be the subject of a future article.
\end{abstract}

\tableofcontents{}

\section{Introduction}

TODO: conceptuellement simple, mais beaucoup d'obstacles techniques pour rendre ça possible.

 Destinations bring a taste of imperative programming in a pure functional environnement when performance really matters, without breaking memory safety.

Using destinations as a way of allocating and building functional data structures can lead to better time and/or space performance for critical parts of a program, but destinations also increase expressiveness of a functional language.

\subsection{Problem space and motivation}

\subsection{Framework}

- GHC where purity is enforced

- Also because it has good support for linear type discipline

TODO: rappel théorique

- Linear types
- Destinations
- Compact regions
- strict lazy haskell

(préliminaire car ça existe déjà)

Article à destination des experts haskell ? CamL?

\section{Motivating examples for destination-style programming}

\subsection{(Perf) Efficient Difference-list implementation}

In functional programming languages, the most used sequence type is the linked list, because its structure makes it very natural to process in a functional/recursive fashion. However, linked lists have rather poor performance for two common operations on sequences: indexing, and concatenation.

To concatenate two linked list in an immutable setting, it is necessary to first copy every node of the first list, and just change the last \mintinline{haskell}`xn : []` \emph{cons} cell by \mintinline{haskell}`xn : ys` where \mintinline{haskell}`ys` stands for the second list. Hence a $\mathcal{O}(n_1)$ operation where $n_1$ stands for the length of the first list.

As a result, the performance penalty incurred by linked list's concatenation can become substantial, especially when that operation is frequently used in a program. Instead, one might opt for an alternative sequence type, \emph{difference lists}, which are fast to convert into lists ($\mathcal{O}(1)$ TODO: really?) but are much more efficient for concatenation.

A difference list is a linked list which isn't terminated by \emph{nil}. Instead, the second component of the last \emph{cons} cell is left pending. In Haskell, an incomplete structure cannot be built in memory strictly speaking, but instead we can represent a difference list \mintinline{haskell}`[x1, ..., xn, ?]` by a function taking a last element \mintinline{haskell}`ys` of type `[a]` and returning \mintinline{haskell}`x1 : ... : xn : ys` (having type \mintinline{haskell}`[a]` too).

With such representation, concatenation is just function composition: \mintinline{haskell}`f1 <> f2 = f1 . f2`, and we have \mintinline{haskell}`mempty = id`, \mintinline{haskell}`toList f = f []` and \mintinline{haskell}`fromList xs = \ys → xs ++ xs`. This is exactly what is implemented in the difference list library for Haskell, that is especially useful for logging or string building programs (as they tend to be concatenation-heavy).

With destinations in hand, we can now build actual incomplete structures in memory in a strict setting. A difference list can be seen as a pair of a list \mintinline{haskell}`[a]`, and a destination of a list \mintinline{haskell}`Dest [a]` (that will receive the last element of the list). The actual list \mintinline{haskell}`[a]` becomes readable as soon as the last element is written to it. In other terms, to free the list \mintinline{haskell}`[a]`, the accompanying destination must be linearly consumed. This constraint is embodied by the type \mintinline{haskell}`type DList a = Incomplete [a] (Dest [a])` which is at the core of our destination API.

The other operations on difference lists follow easily:
\begin{minted}[linenos]{haskell}
type DList a = Incomplete [a] (Dest [a])

new :: DList a
new = alloc

append :: DList a ⊸ a → DList a
append i x =
  i <&> \dl → case dl & fill @'(:) of
    (dh, dt) → dh & fillLeaf x `lseq` dt

concat :: DList a ⊸ DList a ⊸ DList a
concat i1 i2 = i1 <&> \dl1 → dl1 & fillComp i2

toList :: DList a ⊸ [a]
toList i = extractFromReg $ i <&> \dl → dl & fill @'[]
\end{minted}

\begin{itemize}
  \item \mintinline{haskell}`new` calls \mintinline{haskell}`alloc` which just returns a hollow \mintinline{haskell}`Incomplete [a] (Dest [a])` structure: there is no data there yet, so the list that will be fed in the \mintinline{haskell}`Dest [a]` is exactly the list in that the \mintinline{haskell}`Incomplete` will liberate.
  \item \mintinline{haskell}`append` fills the destination (that stands for the hole at the end of the list) with a new \emph{cons} cell. The head of that cell \mintinline{haskell}`dh` is filled with the value to append, and the remaining hole (represented by `dt :: Dest [a]`) is the hole of the resulting difference list.
  \item \mintinline{haskell}`concat` calls \mintinline{haskell}`fillComp`, which fills the destination of the first difference list with the root of the second one. The resulting \mintinline{haskell}`Incomplete` object hence has the same root as the first list, holds the elements of both lists, and has the hole of the second list at the end. Memory-wise, `concat' will just write the address of the root of the second list into the hole of the first one ; no move is required.
  \item \mintinline{haskell}`toList` completes the difference list by plugging \emph{nil} into its hole.
\end{itemize}

As it is shown with this example, thanks to destination-style programming, not only we can express programs and functions whose implementation is closer to their intended memory behaviour (here, implementing data structures with holes), but we can also get important performance improvements as a side benefit.

[Insert benchmark here].

\subsection{(Expressiveness) BFS Tree mapping/traversal}

Traversing a binary tree in a breadth-first fashion with effects is notoriously hard to implement efficiently in a pure functional fashion.

Indeed, the order in which effects should be sequenced is completely different than the natural order of traversal of the tree. As a result, many tricks should be employed to reconciliate those two realities. Even the most elegant solutions such as [introduce ref to the now/later applicative implementation] often involve several traversals of the tree, or the construction of additional cost-heavy structures. 

With destinations in our toolbelt, we can implement a solution that is both easy to write and efficient, doing only a single traversal pass on the original tree.

The main idea is to keep a queue of pairs, whose first element is the next node to process, and the second is a destination in the output tree where to write the result of processing that node. Thanks to destination, it is possible to let some parts of the tree "unfinished" for some time, and to come back at them later when it's their turn to be processed. That way, effects can be applied in a BFS order and the tree can be built in one pass. In a way, destinations provide a one-use \emph{action-at-a-distance} concept in this use-case.

\begin{minted}[linenos]{haskell}
data Tree a = Nil | Node a (Tree a) (Tree a)

mapStateBFS :: ∀ a b s. (s → a → (s, b)) → s → Tree a → (s, Tree b)
mapStateBFS f s0 tree =
  buildInRegionAndExtract $
    \token → alloc token <&>
      \dtree → go s0 (singleton (Ur tree, dtree))
  where
    go :: s → Queue (Ur (Tree a), Dest (Tree b)) ⊸ Ur s
    go s q = case dequeue q of
      Nothing → Ur s
      Just ((utree, dtree), q') → case utree of
        Ur Nil → dtree & fill @'Nil `lseq` go s q'
        Ur (Node x tl tr) → case dtree & fill @'Node of
          (dr, dtl, dtr) →
            let q'' = q' `enqueue` (Ur tl, dtl) `enqueue` (Ur tr, dtr)
                (s', r) = f s x
              in dr & fillLeaf r `lseq` go s' q''
\end{minted}

The signature of `mapStateBFS' doesn't involve linear types. They are only put into use in the implementation of the said function, as a way to ensure memory safety when using destinations.
Because the state-transforming function `(s → a → (s, b))' is non-linear, leaves of the original tree aren't consumed in a linear fashion. However, the queue in which nodes and destinations are stored must be linearly consumed: it's only when all destinations (and by extension, their container, i.e. the queue in this context) have been linearly consumed that the built structure will be "released" and will be readable.

As a result, nodes of the tree stored in the queue must be wrapped in `Ur', to indicate that the queue can still be linearly consumed even though the tree nodes are consumed in a non-linear fashion. On the other hand, the destinations cannot be wrapped in `Ur', and as a result their linear consumption is necessary for the queue to be considered as linearly consumed.

The first two lines of the function are just implementation noise to obtain a destination of a tree `dtree'. Then the `go' helper function is called on the initial state with a single-element queue containing the root of tree, and the allocated destination for the resulting tree.

With this example, we show how destinations can be used in a non-linear setting to improve the expressiveness of the base language. That being said, they also tend to improve performance as a side benefit, compared to other elegant functional implementations: [INSERT HERE BENCHMARK RESULT].

Being able not only to represent data structures with holes, but also manipulate references to these holes as first-class objects of the language while preserving memory safety, is the major step forward that this paper presents. Earlier work from Minamide presented a memory-safe functional language supporting data structures with (just) a hole. It would have been possible to extend that preliminary work to support multiple holes rather easily, but being able to reference these holes without having the associated incomplete structure at hand is where the real deal is.

\subsection{(Perf) Deserializing from a structured format (S-Expr)}

Derserialization is a very common task in application development, and can become a crucial part of the program performance-wise, when the program in question is meant to be fed with large chunks of serialized data.

Let's focus on a deserializer for S-expressions. S-expresssions are parenthesized lists whose elements are just seperated by spaces. These elements can be of several types: int, string, symbol (a textual token, with no quotes around it, unlike a string), or a list of other S-expressions.

Parsing a S-expression can be done concisely with three mutually recursive functions:
\begin{itemize}
  \item \mintinline{haskell}`parseSExpr` scans the next character, and either dispatches to \mintinline{haskell}`parseSList` if it encounters an opening parenthesis, or to \mintinline{haskell}`parseSString` if it encounters an opening quote, or eventually parses the string into a number or symbol;
  \item \mintinline{haskell}`parseSList` calls \mintinline{haskell}`parseSExpr` to parse the next token, and then calls itself again until reaching a closing parenthesis, accumulating the parsed elements along the way;
  \item \mintinline{haskell}`parseSString` scans the input character by character and accumulates them until reaching a closing quote (taking escape sequences into consideration).
\end{itemize}

\begin{minted}[linenos]{haskell}
parseSExpr :: ByteString → Int → Either ParseError SExpr
parseSExpr bs i = case bs !? i of
  Nothing → Left $ UnexpectedEOFSExpr i
  Just c → case c of
    ')' → Left $ UnexpectedClosingParen i
    '(' → parseSList bs (i + 1) []
    '"' → parseSString bs (i + 1) False []
    _ →
      let tok = extractNextToken bs i -- take chars until delimiter/space
       in if null tok
            then -- c is a (leading) space, skip it and recurse
              parseSExpr bs (i + 1)
            else case parseInt tok of
              Just int → Right $ SInteger (i + length tok - 1) int
              Nothing → Right $ SSymbol (i + length tok - 1) (toString tok)

parseSList :: ByteString → Int → [SExpr] → Either ParseError SExpr
parseSList bs i acc = case bs !? i of
  Nothing → Left $ UnexpectedEOFSList i
  Just c → \cases
    | c == ')' → Right $ SList i (reverse acc)
    | isSpace c → parseSList bs (i + 1) acc
    | otherwise → case parseSExpr bs i of
        Left err → Left err
        Right child → parseSList bs (endPos child + 1) (child : acc)

parseSString :: ByteString → Int → Bool → [Char] → Either ParseError SExpr
parseSString bs i escape acc = case bs !? i of
  Nothing → Left $ UnexpectedEOFSString i
  Just c → case c of
    '"' | not escape → Right $ SString i (reverse acc)
    '\\' | not escape → parseSString bs (i + 1) True acc
    'n' | escape → parseSString bs (i + 1) False ('\n' : acc)
    _ → parseSString bs (i + 1) False (c : acc)
\end{minted}

Now, this implementation is already quite efficient for a strict setting. The functions are as tail-recursive as they can be, and they use indexing and slicing into a bytestring ($\mathcal{O}(1)$ operation) instead of potentially allocating a huge number of strings on the heap.

That being said, it is possible to obtain very significative performance gains by using destinations with only very little stylistic changes in the code.

Accumulators of tail-recursive functions just have to be changed into destinations. Instead of writing elements into a list that will be reversed at the end as we did before, the program in the destination style will directly write the elements into their final location.

The current implementation of destination-style programming for Haskell is based on Compact Regions, which have two strong properties that clash with usual Haskell ones. First, strictness is mandatory, and any object written in a compact region will be forced into its normal form. Secondly, all objects from a same region will share the same lifetime. Those properties must be dealbreaker for some use cases, but for a parser, they very advantageous!

Indeed, in a program that consumes raw data and parses it (and for which performance does matter), it's uncommon for a part of the input, in its parsed but unprocessed form, to greatly outlive the rest of it. As a result, treating the deserialized data as a large object with a unique lifetime for all its parts is a decent approximation that then allows easier memory management, and as a result impressive garbage collection gains!

As for strictness, most of the time, the result of deserialization will be a data tree, with hundred of nodes and leaves, whose consumption by the processing program will be strict because:
\begin{itemize}
  \item the programmer wants to catch any potential error in the input document early enough in the process;
  \item most of the input will be read to produce the output anyway.
\end{itemize}

All things considered, the properties imposed by the use of destinations seem very compatible with a parsing use-case.

It is good to note that destination allow to reverse the natural order in which a structure is built. For a list, the natural operation in functional programming languages is \emph{prepend}/\emph{cons}, which adds an element at the front of an existing list (bottom-up approach). Thanks to destinations, it's possible to build a list starting from an element which will stay at the head of the it, and new elements will be added towards the tail of the list (\emph{append}/\emph{fillCons}). It's entirely possible to mix both approaches too, as it will be detailed in the API section.

Let's focus on the implementation using destinations:

\newcommand{\mnew}[1]{\colorbox{green}{#1}}
\newcommand{\mold}[1]{\colorbox{red}{\sout{#1}}}

\begin{minted}[escapeinside=°°,linenos]{haskell}
parseSExprDS :: ByteString → Int → Dest SExpr ⊸ Either ParseError Int
parseSExprDS bs i d = case bs !? i of
  Nothing → °\mnew{d & fillLeaf defaultSExpr}° `lseq` Left $ UnexpectedEOFSExpr i
  Just c → case c of
    ')' → °\mnew{d & fillLeaf defaultSExpr}° `lseq` Left $ UnexpectedClosingParen i
    '(' → parseSListDS bs (i + 1) °\mnew{(d & fill @'SList)}\mold{[]}°
    '"' → parseSStringDS bs (i + 1) False °\mnew{(d & fill @'SString)}\mold{[]}°
    _ →
      let tok = extractNextToken bs i -- take chars until delimiter/space
        in if null tok
            then -- c is a (leading) space, skip it and recurse
              parseSExprDS bs (i + 1) d
            else case parseInt tok of
              Just int →
                °\mnew{d & fill @'SInteger & fillLeaf int}°
                  `lseq` Right (i + length tok - 1)
              _ →
                °\mnew{d & fill @'SSymbol & fillLeaf (toString tok)}°
                  `lseq` Right (i + length tok - 1)

parseSListDS :: ByteString → Int → Dest [SExpr] ⊸ Either ParseError Int
parseSListDS bs i d = case bs !? i of
  Nothing → °\mnew{d & fill @'[]}° `lseq` Left $ UnexpectedEOFSList i
  Just c →
    \cases
      | c == ')' → °\mnew{d & fill @'[]}° `lseq` Right i°\mold{(reverse acc)}°
      | isSpace c → parseSListDS bs (i + 1) d
      | otherwise →
          let !(dh, dt) = °\mnew{d & fill @'(:)}°
          in case parseSExprDS bs i °\mnew{dh}° of
                Left err → dt & fill @'[] `lseq` Left err
                Right endPos → parseSListDS bs (endPos + 1) °\mnew{dt}\mold{(child : acc)}°

parseSStringDS :: ByteString → Int → Bool → Dest [Char] ⊸ Either ParseError Int
parseSStringDS bs i escape d = case bs !? i of
  Nothing → °\mnew{d & fill @'[]}° `lseq` Left $ UnexpectedEOFSString i
  Just c → case c of
    '"' | not escape → °\mnew{d & fill @'[]}° `lseq` Right i°\mold{(reverse acc)}°
    '\\' | not escape → parseSStringDS bs (i + 1) True d
    'n' | escape →
        let !(dh, dt) = °\mnew{d & fill @'(:)}°
         in °\mnew{dh & fillLeaf '\textbackslash{}n'}° `lseq` parseSStringDS bs (i + 1) False °\mnew{dt}\mold{('\textbackslash{}n' : acc)}°
    _ →
        let !(dh, dt) = °\mnew{d & fill @'(:)}°
         in °\mnew{dh & fillLeaf c}° `lseq` parseSStringDS bs (i + 1) False °\mnew{dt}\mold{(c : acc)}°
\end{minted}


\begin{itemize}
  \item Even for error cases, we are forced to consume the destination that we receive as an argument, hence we write some sensible default data to it (see lines 3, 5, 23 and 36)
  \item Because of the top-down approach, it's necessary to choose which variant of SExpr is being built very early (by writing the corresponding constructor into the \mintinline{haskell}`SExpr` destination), instead of building an accumulator and wrapping the variant constructor over it as it was done in the naive implementation (see lines 6 and 7);
  \item The SExpr value resulting from the call of \mintinline{haskell}`parseSExpr` inside \mintinline{haskell}`parseSList` is no longer collected by the caller; but instead written directly into its final location by the callee (see lines 30 to 32).
  \item Adding an element \mintinline{haskell}`a` to the accumulator \mintinline{haskell}`[a]` is replaced with adding a new cons cell with \mintinline{haskell}`fill @'(:)`, writing the element to the head destination (\mintinline{haskell}`Dest a`), and then continuing the processing with the tail destination (\mintinline{haskell}`Dest [a]`) (see lines 29, 32, 42 and 45).
  \item Instead of reversing and returning the accumulator at the end of the processing, it is enough to complete the list by writing a nil element to the tail destination (with \mintinline{haskell}`fill @'[]`) (see lines 26 and 38).
\end{itemize}

Thanks to that new implementation, the program runs almost twice as fast as the previous one, mostly because garbage-collection time goes to almost zero. The detailed benchmark is available in section [INSERT SECTION].

\section{Technical development}

\subsection{API Design}

A destination represents a hole in a data structure, and allows for that hole to be filled even if the 

The main design principle behind destination-style structure building is that no structure can be read before all its destinations have been written to. That way, incomplete data structures can be freely passed around and stored, but need to be completed before any pattern-matching can be made on them.

Hence we introduce a new data type \mintinline{haskell}`Incomplete r a b` where \mintinline{haskell}`a` stands for the type of the structure being built, and \mintinline{haskell}`b` is the type of what needs to be linearly consumed before the structure can be read.

Types aren't linear by themselves in Linear Haskell. Instead, functions can be made linear or not, and linearity of resources are ensured through scope-functions: functions taking a callback that linearly consumes the resource (very much like continuation-passing style).

TODO: Il faut que le seul moyen d'avoir resource en position positive soit en CPS avec callback linéaire (ou dans des fonctions qui ont une resource en position négative aussi)

Let's make things clearer with an example:
\begin{minted}{haskell}
data Resource

withResource :: (Resource ⊸ a) ⊸ a
\end{minted}

If \mintinline{haskell}`withResource` is the only producer of \mintinline{haskell}`Resource`, then the only way to ever access a resource will be to supply a linear callback to \mintinline{haskell}`withResource`. Still, this is not enough; because \mintinline{haskell}`\x → x` is indeed a linear callback, one could use \mintinline{haskell}`withResource (\x → x)` to leak a \mintinline{haskell}`Resource`, and then use it in a non-linear fashion.

We must force the callback to actually consume the resource, and not leak it to the outside. To forbid the resource from appearing anywhere in the return type of the callback, we will ask the return type to be wrapped in \mintinline{haskell}`Ur`. Putting something in \mintinline{haskell}`Ur` is a non-linear operation, except for \mintinline{haskell}`Movable` types, which are basic ones (\mintinline{haskell}`Int`, \mintinline{haskell}`String`, \mintinline{haskell}`Char`...) and structures made of them. As linear resource is simply a data structure which doesn't implement \mintinline{haskell}`Movable`, and which cannot be wrapped linearly in \mintinline{haskell}`Ur`.

With the following declaration, our linear resource cannot leak to the outside world, and must be consumed linearly by the callback (using other functions supplied by the API, written in direct style this time):

\begin{minted}{haskell}
class Movable a where
  move :: a ⊸ Ur a

data Resource

withResource :: (Resource ⊸ Ur a) → Ur a
updateResource :: Int ⊸ Resource ⊸ Resource
closeResource :: Resources ⊸ ()

-- OK
withResource (\r → r & updateResource 42 & closeResource & move) :: Ur ()

-- fails with linearity error
withResource (\r → r & move) :: Ur Resource
\end{minted}

This is mostly the design principle that have been used for destinations in Haskell. In order to access the \mintinline{haskell}`Incomplete`'s \mintinline{haskell}`a` value, the \mintinline{haskell}`b` side must be transformed/consumed into something with type \mintinline{haskell}`Ur c`. Because the \mintinline{haskell}`b` side hosts the destinations initially, they have to be consumed by a linear callback mapping on \mintinline{haskell}`b` side before \mintinline{haskell}`fromReg` can be used to access the \mintinline{haskell}`a`. As we explained above, they cannot leak as there is no linear way to produce a \mintinline{haskell}`Ur Dest` from a \mintinline{haskell}`Dest`.

\begin{minted}{haskell}
newtype Incomplete r a b = Incomplete (a, b)

instance Control.Functor (Incomplete r a) where
  fmap :: (b ⊸ c) ⊸ Incomplete r a b ⊸ Incomplete r b c
  fmap f (Incomplete (s, d)) = Incomplete (s, f d)

fromRegExtract :: ∀ r a b. (RegionContext r) ⇒ Incomplete r a (Ur b) ⊸ Ur (a, b)
\end{minted}

\mintinline{haskell}`Region`, \mintinline{haskell}`RegionContext r` and \mintinline{haskell}`RegionToken r` are mostly implementation noise for the API. There presence will be justified in the next subsection.

Allocation a new receiver of type \mintinline{haskell}`a` is done through \mintinline{haskell}`alloc`:

\begin{minted}{haskell}
alloc :: ∀ r a. RegionToken r ⊸ Incomplete r a (Dest r a)
\end{minted}

This function signature can be read that way : it consumes a region token, and returns an object holder, which upon consumption of a \mintinline{haskell}`Dest r a`, will unlock the object of type \mintinline{haskell}`a`. At this point, the return value of `alloc` is pretty much the identity: give it an \mintinline{haskell}`a` (to fill the \mintinline{haskell}`Dest r a`), and it will return an \mintinline{haskell}`a`.

To fill that hole represented by \mintinline{haskell}`Dest r a`, several functions are available:

\begin{itemize}
  \item \mintinline{haskell}`fillLeaf :: ∀ r a. (RegionContext r) ⇒ a → Dest r a ⊸ ()` \\will use a non-linear value of type \mintinline{haskell}`a` to fill the hole;
  \item \mintinline{haskell}`fillComp :: ∀ r a b. (RegionContext r) ⇒ Incomplete r a b ⊸ Dest r a ⊸ b` \\will use an incomplete object of type \mintinline{haskell}`a` to fill the hole; and the resulting holes for the bigger structure will be the ones of that object. In other terms, \mintinline{haskell}`fillComp` composes two structures with holes together;
  \item \mintinline{haskell}`fill :: ∀ liftedCtor r a. Dest r a ⊸ DestsOf liftedCtor r a` \\takes a constructor as a type parameter (\mintinline{haskell}`liftedCtor`) and will write a shallow instance of that constructor into the \mintinline{haskell}`Dest r a`, returning the destinations corresponding to that constructor's fields.\\ For example, \mintinline{haskell}`fill @Just @r @(Maybe Int) :: Dest r (Maybe Int) ⊸ Dest r Int` writes a shallow \mintinline{haskell}`Just` constructor in the \mintinline{haskell}`Maybe Int` hole, and returns the \mintinline{haskell}`Dest Int` corresponding to the hole of type \mintinline{haskell}`Int` that still needs to be filled.
\end{itemize}

\mintinline{haskell}`fill` is probably the most interesting of the three, and will be the most used one too, because it enables the user to build data structures in a top-down approach, which complements very well the natural bottom-up way of constructing data structures in functional programming languages. Thanks to destinations, the user can now choose between those two approaches and pick the most efficient or more natural way for the problem at hand.

\mintinline{haskell}`fillComp` offers a way to mix both approaches: one can build small chunks in a top-down approach, and combine them together in a bottom-up fashion even if they aren't all complete.

\mintinline{haskell}`fillLeaf` is just a restriction of \mintinline{haskell}`fillComp`. It is used very frequently for types whose constructors aren't \mintinline{haskell}`Fill`able because they wrap over unpacked or primitive fields.

\subsection{Note about Compact Regions}

At the moment, destination-style programming is only possible in compact regions. Compact regions are special chunks on the heap that will only be very lightly inspected by the garbage collector, and thanks to that, we can do chirurgical memory updates in those without being afraid that it will interfere with garbage collection (especially move operations).

Because we have immobile chunks of memory, destinations can be implemented as a wrapper over a raw pointer which points to the memory location where data have to be written:

\begin{minted}{haskell}
data Dest r a = Dest Addr#
\end{minted}

The phantom type parameter \mintinline{haskell}`r` that is present everywhere in the API ensures that objects can only interact with other ones from the same region (as outgoing pointers across different regions are not allowed by design of Compact regions).

A \mintinline{haskell}`Region` object is implemented as a newtype over \mintinline{haskell}`Compact FirstInhabitant`, which is composed of a pointer to the region header, a lock (because compact region writes are not thread-safe at the primitive level), and a pointer to a \mintinline{haskell}`FirstInhabitant` inside the region.

\mintinline{haskell}`RegionToken r` carries a non-linear \mintinline{haskell}`Region` and is used when a \mintinline{haskell}`Proxy r` would be necessary anyway to infer the \mintinline{haskell}`r` of \mintinline{haskell}`Dest`/\mintinline{haskell}`Incomplete` in a function signature. \mintinline{haskell}`RegionContext r` makes the region argument implicit when the region identifier \mintinline{haskell}`r` is already part of the types of the function arguments.

\subsection{User-land haskell implementation details}
 
One issue I had during implementation was choosing the right representation for \mintinline{haskell}`a` in \mintinline{haskell}`Incomplete r a b`.

Ideally, we want \mintinline{haskell}`Incomplete r a b` to contains a \mintinline{haskell}`a` and a \mintinline{haskell}`b`, and let the \mintinline{haskell}`a` free when the \mintinline{haskell}`b` is fully consumed (or linearly transformed into \mintinline{haskell}`Ur c`). So the most straightforward representation of Incomplete would be a pair \mintinline{haskell}`data Incomplete r a b = Incomplete a b`.

It is also natural for \mintinline{haskell}`alloc` to return an \mintinline{haskell}`Incomplete r a (Dest a)`: as soon as the \mintinline{haskell}`Dest a` is linearly consumed, a \mintinline{haskell}`a` will be available.

However, building something of type \mintinline{haskell}`Incomplete r a (Dest a)` is not easy. The \mintinline{haskell}`Dest a` in question should carry the address of a memory cell in which the address of a constructor of type \mintinline{haskell}`a` will be written (by \mintinline{haskell}`fill` or a similar function). That memory cell is named \emph{root receiver}, because it will receive the address of the root of the object being built. Given the data definition above, the root receiver should hence be the first field of the \mintinline{haskell}`Incomplete` object. But the Incomplete object is not part of the compact region itself! It lives in the normal GC heap (which is crucial for early deallocation and memory optimizations); so it may move. As a result, we cannot just use the address of its left field as a destination and the left field as a root receiver.

The next natural move is to allocate a wrapper \mintinline{haskell}`W a` for \mintinline{haskell}`a` in the region, that will act as a root receiver, and have \mintinline{haskell}`data Incomplete r a b = Incomplete (W a) b`. Because the \mintinline{haskell}`a` will have to be wrapped in \mintinline{haskell}`Ur` when it is complete (see \mintinline{haskell}`fromReg :: ∀ r a. (RegionContext r) ⇒ Incomplete r a () ⊸ Ur a`), we might as well use \mintinline{haskell}`Ur` as a wrapper and declare \mintinline{haskell}`data Incomplete r a b = Incomplete (Ur a) b`. It solves the previous issue of not having a receiver to write into.

Unfortunately, that approach is quite unsatisfying because any \mintinline{haskell}`Incomplete` object will now allocate a few words into the region that won't be collected for a long time. In particular, putting a non-linear value inside the region with \mintinline{haskell}`intoReg :: ∀ r a. RegionToken r ⊸ a → Incomplete r a ()` will allocate an unnecessary \mintinline{haskell}`Ur` wrapper, as the object is already fully built, and thus doesn't need a root receiver. With the \mintinline{haskell}`Ur` solution, it's no longer efficient to write \mintinline{haskell}`fillLeaf` as a composition \mintinline{haskell}`fillComp . intoReg (getToken @r)`, and \mintinline{haskell}`fillLeaf` needs a dedicated optimized implementation.

Another approach we explored is to represent an \mintinline{haskell}`Incomplete` as a piece of data structure that would be concretized when it is given a root receiver. Delaying the construction of the structure that way is not efficient though, because instead of writing a structure to memory, we create huge closures representing the action of creating the structure.

Finally, I decided to embed the \mintinline{haskell}`a` field in \mintinline{haskell}`Incomplete` not in a regular data constructor wrapper, but instead in a STG indirection closure (\mintinline{haskell}`stg_IND` symbol). The indirection closure is allocated into the compact region in the \mintinline{haskell}`alloc` case (for the same reasons as the \mintinline{haskell}`Ur` above was), but no indirection closure is needed in \mintinline{haskell}`intoReg`, we can just store the fully built \mintinline{haskell}`a` directly in \mintinline{haskell}`Incomplete`. That way, we keep uniformity in the typing and runtime behaviour of \mintinline{haskell}`Incomplete` in both cases (as indirections are transparent for the RTS), and ensure that the overhead of `fillLeaf`/\mintinline{haskell}`intoReg` is very small, at the cost of a few more alloc-ed words for each call of \mintinline{haskell}`alloc`. This is a fair price to pay because any real-world program should call \mintinline{haskell}`fillLeaf` way more than \mintinline{haskell}`alloc` (which is only required to create the root of a new big object).

The implementation of \mintinline{haskell}`fromReg` and \mintinline{haskell}`fromRegExtract` is then relatively straightforward. \mintinline{haskell}`fromReg :: forall r a. (RegionContext r) => Incomplete r a () %1 -> Ur a` discards the \mintinline{haskell}`()` value carried by the Incomplete, allocates a \mintinline{haskell}`Ur _` in the region, writes the address of the \mintinline{haskell}`a` value into the \mintinline{haskell}`Ur` slot, and returns it. \mintinline{haskell}`fromRegExtract :: forall r a b. (RegionContext r) => Incomplete r a (Ur b) %1 -> Ur (a, b)` allocates a \mintinline{haskell}`Ur (_, _)` into the region, checks whether the \mintinline{haskell}`b` in \mintinline{haskell}`Ur b` is already part of the region or not (and copies it into the region if it isn't), and writes the address of the \mintinline{haskell}`a` and \mintinline{haskell}`b` values into the slots of the \mintinline{haskell}`Ur`ed pair before returning it.

\paragraph{Deriving \mintinline{haskell}`Fill` for all datatypes with `Generics`}

The \mintinline{haskell}`Fill liftedCtor r a` class has only one method, \mintinline{haskell}`fill :: Dest r a -> DestsOf liftedCtor r a`.

As it has been said before, the action of \mintinline{haskell}`fill @liftedCtor @r @a` is to allocate a new shallow constructor \mintinline{haskell}`Ctor _ :: a` in the region, and fill a destination of type \mintinline{haskell}`Dest r a` with its address. Because the constructor is shallow (i.e. its fields haven't been initialized), \mintinline{haskell}`fill` should also return destination objects pointing to these incomplete fields a.k.a. holes.

The type of the destinations that should be returned by \mintinline{haskell}`fill` can be computed by carefully inspecting the \mintinline{haskell}`Generic` representation of the type \mintinline{haskell}`a`. Indeed, GHC.Generics is a haskell library that provides compile-time inspection of a type metadata (list of constructors, fields, memory representation...) if the type implements the \mintinline{haskell}`Generic` class. Fortunately, the implementation of the \mintinline{haskell}`Generic` class be derived automatically by the compiler (instead of being implemented manually), making it easy to use for user-defined types.

In the \mintinline{haskell}`DestsOf` type family and in in the different \mintinline{haskell}`Fill` instance heads, I inspect the \mintinline{haskell}`Generic` representation of the given type \mintinline{haskell}`a` to find the metadata of the constructor whose lifted representation is \mintinline{haskell}`liftedCtor`, and then I extract the number of fields of the constructor and their types.

For example, the \mintinline{haskell}`Generic representation` of \mintinline{haskell}`Maybe a` gives

\begin{minted}[escapeinside=°°,linenos]{haskell}
M1 D (MetaData "Maybe" "GHC.Maybe" "base" False) (
  M1 C (MetaCons °\mnew{"Nothing"}° PrefixI False) °\mnew{U1}°
  :+: M1 C (MetaCons °\mnew{"Just"}° PrefixI False) (M1 S [...] °\mnew{(K1 R a)}°))
\end{minted}

indicating that there is zero fields with constructor `Nothing' and one field of type `a' with constructor `Just'. 


The representation for `(a, b)' gives

\begin{minted}[escapeinside=°°,linenos]{haskell}
  M1 D (MetaData "Tuple2" "GHC.Tuple.Prim" "ghc-prim" False) (
    (M1 C (MetaCons °\mnew{"(,)"}° PrefixI False) (
      M1 S [...] °\mnew{(K1 R a)}°
      :*: M1 S °\mnew{(K1 R b)}°))
\end{minted}

indicating that there are two fields, one of type \mintinline{haskell}`a` and one of type \mintinline{haskell}`b`, with the \mintinline{haskell}`(,)` data constructor.

Fields of a constructor are stored contiguously in memory, at offset $i \times \textit{wordsize}_{(i \in 1..n)}$ from the constructor base address. So \mintinline{haskell}`fill` just harvests the field metadata and can easily build as many destinations as there are fields, using the address of the newly allocated constructor as a base point. The extracted types of the fields are used to specify the phantom type parameter of the destinations.

\subsection{Changes to GHC's internals and RTS}

The implementation we described in the previous parts relies on being able to allocate shallow constructors. This is the key point of destination-style programming: building structures in a top-down approach, where nodes deeper in the data tree are left unspecified for some time.

Usually, in Haskell, every field must be explicitly specified to allocate a constructor closure. This definitely makes sense as there is no (safe) way to later update a field in a mutable/in-place fashion; so an unspecified field would stay unspecified forever.

Of course, it is possible to overcome that well-founded rule by using a unique \emph{joker value} for every field of a constructor: \mintinline{haskell}`Right undefined` or \mintinline{haskell}`(undefined, undefined)` are valid haskell values that shouldn't blow up too fast. Using an unsafe coercion works too: \mintinline{haskell}`(unsafeCoerce (), unsafeCoerce ()) :: (a, b)` is valid. Theoretically, we hence have a way to allocate a somewhat shallow constructor.

That being said, there is no primitive in GHC at the moment that allows to allocate a value directly in a compact region. The only way to put something in a region is to copy something that has already been allocated in the normal GC heap. So even with the trick described above, the constructor closure would need to be allocated twice.

As a result, in order to obtain the maximum performance and memory gains from the use of destination, I had no other choice than to tweak the GHC compiler and add a new primitive operation (\emph{primop}) in charge of allocating a shallow constructor directly into a compact region.

\paragraph{About GHC architecture, RTS and primops}

GHC is the main compiler for the Haskell programming language, and its compilation model involves several intermediary languages.

First, Haskell code is turned into Core, a typed language whose syntax is mostly a subset of Haskell one. A lot of Haskell constructs are desugared into constructors and case expressions, type applications are made explicit, etc.
Then, several optimization steps are made on the Core output, like removing case expressions on known constructors, or floating \mintinline{haskell}`let` expressions (moving them upwards in the code to factor out some computations).

When all Core optimizations have been made, Core code is then transformed into STG code. In STG, all memory objects (called \emph{closures}) have the same memory representation, in the form of a pointer to a struct called the \emph{info table}, and a payload of a variable size (composed of primitive values and pointers to other closures). Because Haskell works in a lazy fashion, closures might not represent evaluated values, so the info table of each closure contains a pointer to a block of code that is responsible for evaluating the sub-expression that the closure represents. In other terms, the caller is not responsible for knowing how to evaluate the closure, but the closure itself is.
As you can imagine, having a dedicated info table with each closure would be excessively expensive memory-wise (as both of them would need to be dynamically allocated). Instead, the STG uses sharing as most as possible, and for example, every dynamic closure representing the constructor application of a given constructor (let's say \mintinline{haskell}`Just`) will have the same info table pointer, pointing to a single statically-allocated info table. Indeed, no matter what type of argument it is applied to, every \mintinline{haskell}`Just` instance will have the same memory layout (only one boxed argument as a payload), and the same evaluation code (doing nothing and returning immediately because a constructor application is already in weak-head normal form). That kind of sharing in fact quite common in other programming languages too: virtual tables are shared in the same way in C++.

Other specific info tables are also being shared in the STG: the ones for "blackholes" (which is used to indicate that a closure is currently under evaluation by a thread and that its result is still pending); the ones for indirection, etc.

All that sharing is made through the use of labels:

- when the declaration of a data constructor is encountered (definition site), the corresponding info table is emitted to the static area of the output program, and its address is associated with the label \mintinline{haskell}`<constructor name>_con_info`
- when the use of a data constructor is encountered, the label \mintinline{haskell}`<constructor name>_con_info` is used in place of an actual address in the closure that is emitted
- later on in the compilation process, those labels will be converted by the linker into actual addresses

Now, the runtime behaviour of a Haskell program is directed by the \emph{run-time system}, or RTS, which is a software component written in a mix of C and C-- (the last language in the compilation pipeline of Haskell for a native build). The RTS is built once for all when GHC itself is being built, and it is then included in every executable produced by GHC.

Its role is to manage threads, organize garbage collection and also to manage compact regions (among other things). The RTS defines various primops that allow the haskell programmer to interact with it for those needs. For example, \mintinline{haskell}`compactAdd#` is the primitive operation that copies a closure into an existing compact region, fully evaluating its potentiel sub-expressions along the way.

There are also primops in GHC which are meant to be resolved at the \emph{Core to Stg} phase (emitting some code at compile time), but which don't trigger any specific interaction with the RTS at runtime. Among those are all the primitive numeric operators on primitive types (\mintinline{haskell}`+#`, \mintinline{haskell}`-#`...), and more generally all the performance-critical operations working on unboxed types, and which don't require any specific interaction or precaution with garbage collection or thread execution, or don't need a specific allocation scheme, etc.

It is important to note for the next of that part that the RTS cannot really access information that would have been made available during the compilation of a specific haskell program. Indeed, the RTS included in the final executable for the given program is the same as the one included in all other executables made with the same version of GHC ; it isn't possible to parametrize the RTS with data collected during the compilation of the given program (ex: types used or defined in the program). The only data channel I found between the compilation stage of a program and the RTS is to reify the needed compile-time information into a runtime value that will later be passed to the RTS through a primop call.

\paragraph{Find a good title here}

What I want to do is twofold:
\begin{itemize}
\item First, add a primop to the RTS to allocate space into a compact region for a shallow constructor ;
\item Second, add a non-RTS primop to reify the necessary information about a constructor into a runtime-value so that it can be communicated to the RTS (because the RTS cannot access that itself, as I explained above)
\end{itemize}

The first step mostly takes place in \mintinline{text}`rts/Compact.cmm`, the main C-- module of the RTS for compact regions management:

\begin{minted}{haskell}
compactAddShallow# :: Compact# -> Addr# -> State# RealWorld -> (# State# RealWorld, Addr# #)
\end{minted}
\vspace{-0.6cm}
\begin{minted}{c}
stg_compactAddShallowzh(P_ compact, W_ info)
{
    W_ pp, ptrs, nptrs, size, tag, hp;
    P_ to, p;
    again: MAYBE_GC(again);
    STK_CHK_GEN();

    pp = compact + SIZEOF_StgHeader + OFFSET_StgCompactNFData_result;
    ptrs  = TO_W_(%INFO_PTRS(%STD_INFO(info)));
    nptrs  = TO_W_(%INFO_NPTRS(%STD_INFO(info)));
    size = BYTES_TO_WDS(SIZEOF_StgHeader) + ptrs + nptrs;
    p = NULL;  // p isn't actually used by ALLOCATE macro

    ALLOCATE(compact, size, p, to, tag);
    P_[pp] = to;
    SET_HDR(to, info, CCS_SYSTEM);
#if defined(DEBUG)
    ccall verifyCompact(compact);
#endif
    return (P_[pp]);
}
\end{minted}

This function is mostly a glorified call to the \mintinline{haskell}`ALLOCATE` macro defined in the same file, which tries to do a pointer-bumping alloc in the current block of the compact region if there is enough space, and otherwise add a new block to the region.

As we said earlier, \mintinline{haskell}`stg_compactAddShallowzh`/\mintinline{haskell}`compactAddShallow#` needs to access the info table pointer of the constructor we want to allocate, but cannot do so directly as the RTS cannot access STG labels. This is why the function must receive a \mintinline{haskell}`info` parameter of type \mintinline{haskell}`W_`/\mintinline{haskell}`Addr#` (the former is the C-- type; the latter is the Haskell one).

Let's see how to reify the info table pointer of a constructor into a runtime value now. We want to add a new primitive operation in GHC that takes a compile-time-known string or constructor as input and compiles down to the label having that name or corresponding to the constructor's info table pointer.

In Haskell, compile-time-known strings are represented by a type-level string literal of kind \mintinline{haskell}`Symbol`, and constructors can be lifted into type literals as well (with \mintinline{haskell}`LANGUAGE DataKinds` and prefixing the constructor with \mintinline{haskell}`'`, e.g. \mintinline{haskell}`'Just`). So the function we would like to build must have a type parameter somewhere, probably in one of its argument types, corresponding to that type-level literal input, like \mintinline{haskell}`reifyStgInfoPtr# :: Proxy# s -> Addr#` (where \mintinline{haskell}`s` would stand for the lifted constructor or symbol).

The problem is, the compilation pipeline only start emitting labels at the \emph{STG to C--} phase. And at that point, almost all type information for polymorphic primops' parameters have been erased. Fortunately, for some technical reason, information about the actual return type of a primop is retained that late in the compilation process.

Here's the trick I used so: I built a dedicated return type for \mintinline{haskell}`reifyStgInfoPtr#`, namely \mintinline{haskell}`InfoPtrPlaceholder# a`. That type has a phantom type parameter but shares the same memory representation as \mintinline{haskell}`Addr#`. That way, it is possible to use a type hint to provide the type-level literal to the primop: \mintinline{haskell}`reifyStgInfoPtr# (# #) :: InfoPtrPlaceholder# a` will allow the implementation of \mintinline{haskell}`reifyStgInfoPtr#` to read the type parameter \mintinline{haskell}`a` even though it is both phantom and in return position.

Here's the implementation of the primop:

\begin{minted}[escapeinside=°°,linenos]{haskell}
case primop of
  [...]
  ReifyStgInfoPtrOp -> \_ ->  -- we don't care about the function argument (# #)
    opIntoRegsTy $ \[res] resTy -> emitAssign (CmmLocal res) $ °\mnew{case resTy of}°
      -- matches when 'a' is a Symbol, and extracts the symbol value as a FastString in 'sym'
      TyConApp _addrLikeTyCon [_typeParamKind, °\mnew{LitTy (StrTyLit sym)}°] ->
        CmmLit (CmmLabel (°\mnew{mkCmmInfoLabel rtsUnitId (fsLit "stg\_" `appendFS` sym}°)))
      -- matches when 'a' is a lifted data constructor, and extracts it as a DataCon (from GHC.Core.DataCon)
      TyConApp _addrLikeTyCon [_typeParamKind, °\mnew{TyConApp tyCon \_}°] °\mnew{| Just dataCon <- isPromotedDataCon_maybe tyCon}° -> do
        CmmLit (CmmLabel (°\mnew{mkConInfoTableLabel (dataConName dataCon) DefinitionSite}°))
      -- return garbage data when no pattern matches
      _ -> [...]
  [...]
\end{minted}

Going from the \mintinline{haskell}`InfoPtrPlaceholder# a` returned by \mintinline{haskell}`reifyStgInfoPtr#` to an actual \mintinline{haskell}`Addr#` is just a matter of calling the \mintinline{haskell}`unsafeCoerceAddr` function supplied by GHC, as both type are represented by pointers/addresses under the hood.

With both primops in hand, we can allocate a shallow constructor closure directly in a compact region in an efficient fashion. For example, for \mintinline{haskell}`Just`, one should do:
\begin{minted}{haskell}
shallowJust :: Maybe a
shallowJust = compactAddShallow#
  compactRegion#
  (unsafeCoerceAddr (reifyStgInfoPtr# (# #) :: InfoPtrPlaceholder# 'Just))  
\end{minted}

It would probably be possible to merge the two primops into a two-stage one (with both a compile-time and run-time action) without too much effort.

\paragraph{Destination-style programming with stock GHC}

During the early days of my research, I first experimented with shallow allocation without any modification to GHC codebase. Instead of using the primops I described above, I made heavy use of runtime heap inspection and `GHC.Generics' trickery to build shallow constructors with the correct info table pointers. As a result, it is still possible to use destinations-style programming without using a patched version of GHC, but performance gains are not guaranteed in that context.

TODO: si temps restant, comparer les performances

\section{Evaluating performance of destination-style programming}

\subsection{Benchmarking / profiling strategy}

Benchmarking code using destination hasn't been easy.

First, the current implementation of destinations for Haskell uses compact regions, which force strictness "for free", whereas regular Haskell code is lazy by default; and thus the produced data structures need to be explicitly forced into normal form.

There are two main ways to force a structure into normal form. The first one is to use the tools supplied by the `deepseq` package. That packages defines a typeclass, `NFData', which can be easily derived for user-defined datatypes, which in turn gives access to functions like `force' or `rnf' that will force every thunk (pending lazy computation) from a value/structure. Using `deepseq' to benchmark the naive implementation (the one without destinations) has the advantage of not making unnecessary allocations, so the reported memory usage is accurate, but the \emph{forcing} operation might incur extra CPU usage and thus slow the program down, because forcing thunk artificially with `force' or `rnf' is not as efficient as if they were forced in an organic way by the RTS during the program execution.

The second way to force a lazy structure into normal form is to copy it into a compact region. Indeed, the `compact'/`compactAdd' functions from `GHC.Compact' will efficiently force every thunk of a structure before writing their final value into the region. Most of the time, using `compact' is faster than using `force', as the  \emph{forcing} code is written in C/C-- instead of Haskell, but it creates two issues:

\begin{itemize}
  \item the space allocated in the region for the result is counted towards the total memory allocated by the program. However, compact regions aren't suited for all use-cases. For some of them, the GC gains they would offer wouldn't balance for the extra time during which some garbage will continue to live in memory. As a result, it seems unfair to count the extra memory usage in the region incurred by `compact' for programs which wouldn't make use of compact regions in normal time (as the result structure is already present in the normal GC heap) ;
  \item `compact' not only forces all thunks, but also copies their final value into the region. The time required to make the copy, despite being small, is still counted towards the total execution time of the program.
\end{itemize}

To my knowledge, there is no better way than these two to mesure the time taken by a strict program and by a lazy program, in an accurate and fair way.

- Three modes (-T, -s, -p)

- How to evaluate naive code (why we use both copyIntoReg and force)

\subsection{Performance of map implementation (in a strict context)}

- How this is important for Ocaml

\subsection{Performance of the SExpr parser}

\subsection{Performance of BFS Tree traversal}

\subsection{Qualitative evaluation of destination code VS naive code}

- TODO: implement a naive implement of functional mapMBFS

\section{Conclusion and related work}

- Why it's an improvement over Minamide

- Lifting the non-linear restriction for elements stored in dest-allocated structures (= requires more theoretical work)

- Using destinations in different contexts than compact regions (normal GC heap, other kinds of chunk-allocated memory)

\printbibliography

\end{document}
